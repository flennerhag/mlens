{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\nGetting started\n===============\n\nThis tutorial highlights the basics of the\nhigh-level API for ensemble classes, the model selection suite and\nfeatures visualization.\n\n============================  =================================================\n                   Tutorials                                            Content\n============================  =================================================\n`ensemble-guide`         How to build, fit and predict with an ensemble\n`model-selection-guide`  How to compare several estimators in one go\n`visualization-guide`    Plotting functionality\n============================  =================================================\n\nThe `advanced high-level API tutorials <ensemble-tutorial>` shows how to\nleverage advanced features such as probabilistic layers, feature propagation\netc. For tutorials on low-level mechanics, see\n`the mechanics guides <learner_tutorial>`.\n\n\nPreliminaries\n-------------\n\nWe use the following setup throughout:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport numpy as np\nfrom pandas import DataFrame\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\n\nsys.stderr = sys.stdout\n\nseed = 2017\nnp.random.seed(seed)\n\ndata = load_iris()\nidx = np.random.permutation(150)\nX = data.data[idx]\ny = data.target[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nEnsemble guide\n--------------\nBuilding an ensemble\n^^^^^^^^^^^^^^^^^^^^\nInstantiating a fully specified ensemble is straightforward and requires\nthree steps: first create the instance, second add the intermediate layers, and\nfinally the meta estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# --- Build ---\n# Passing a scoring function will create cv scores during fitting\n# the scorer should be a simple function accepting to vectors and returning a scalar\n\nensemble = SuperLearner(scorer=accuracy_score, random_state=seed, verbose=1)\n\n# Build the first layer\nensemble.add([RandomForestClassifier(random_state=seed), SVC()])\n\n# Attach the final meta estimator\nensemble.add_meta(LogisticRegression())\n\n# --- Use ---\n\n# Fit ensemble\nensemble.fit(X[:75], y[:75])\n\n# Predict\npreds = ensemble.predict(X[75:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check the performance of estimator in the layers, call the ``data``\nattribute. The attribute can be wrapped in a :class:`pandas.DataFrame`,\nbut prints in a tabular format as is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Fit data:\\n%r\" % ensemble.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To round off, let's see how the ensemble as a whole fared.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Prediction score: %.3f\" % accuracy_score(preds, y[75:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-layer ensembles\n^^^^^^^^^^^^^^^^^^^^^\n\nWith each call to the ``add`` method, another layer is added to the ensemble.\nNote that all ensembles are *sequential* in the order layers are added. For\ninstance, in the above example, we could add a second layer as follows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble = SuperLearner(scorer=accuracy_score, random_state=seed)\n\n# Build the first layer\nensemble.add([RandomForestClassifier(random_state=seed), LogisticRegression()])\n\n# Build the second layer\nensemble.add([LogisticRegression(), SVC()])\n\n# Attach the final meta estimator\nensemble.add_meta(SVC())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now fit this ensemble in the same manner as before:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble.fit(X[:75], y[:75])\npreds = ensemble.predict(X[75:])\nprint(\"Fit data:\\n%r\" % ensemble.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Model selection guide\n ---------------------\n\n .. currentmodule:: mlens.model_selection\n\n The work horse class is the :class:`Evaluator`, which allows you to\n grid search several models in one go across several preprocessing pipelines.\n The evaluator class pre-fits transformers, thus avoiding fitting the same\n preprocessing pipelines on the same data repeatedly. Also, by fitting all\n models over all parameter draws in one operation, parallelization is\n maximized.\n\n#############################################################################\n The following example evaluates a `Naive Bayes`_ estimator and a\n `K-Nearest-Neighbor`_ estimator under three different preprocessing scenarios:\n no preprocessing, standard scaling, and subset selection.\n In the latter case, preprocessing is constituted by selecting a subset of\n features.\n\n The scoring function\n ^^^^^^^^^^^^^^^^^^^^\n .. currentmodule:: mlens.metrics\n\n An important note is that the scoring function must be wrapped by\n :func:`make_scorer`, to ensure all scoring functions behave similarly regardless\n of whether they measure accuracy or errors. To wrap a function, simple do:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.metrics import make_scorer\naccuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. currentmodule:: mlens.model_selection\n\nThe ``make_scorer`` wrapper\nis a copy of the Scikit-learn's :func:`sklearn.metrics.make_scorer`, and you\ncan import the Scikit-learn version as well.\nNote however that to pickle the :class:`Evaluator`, you **must** import\n``make_scorer`` from ``mlens``.\n\nA simple evaluation\n^^^^^^^^^^^^^^^^^^^\n\nBefore throwing preprocessing into the mix, let's see how to evaluate a set of\nestimator. First, we need a list of estimator and a dictionary of parameter\ndistributions that maps to each estimator. The estimators should be put in a\nlist, either as is or as a named tuple (``(name, est)``). If you don't name\nthe estimator, the :class:`Evaluator` will automatically name the model as the\nclass name in lower case. This name must be the key in the parameter\ndictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.model_selection import Evaluator\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import randint\n\n# Here we name the estimators ourselves\nests = [('gnb', GaussianNB()), ('knn', KNeighborsClassifier())]\n\n# Now we map parameters to these\n# The gnb doesn't have any parameters so we can skip it\npars = {'n_neighbors': randint(2, 20)}\nparams = {'knn': pars}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now run an evaluation over these estimators and parameter distributions\nby calling the ``fit`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(accuracy_scorer, cv=10, random_state=seed, verbose=1)\nevaluator.fit(X, y, ests, params, n_iter=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full history of the evaluation can be found in ``cv_results``. To compare\nmodels with their best parameters, we can pass the ``results`` attribute to\na :obj:`pandas.DataFrame` or print it in tabular format as is:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Score comparison with best params founds:\\n\\n%r\" % evaluator.results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n^^^^^^^^^^^^^\n\nNext, suppose we want to compare the models across a set of preprocessing pipelines.\nTo do this, we first need to specify a dictionary of preprocessing pipelines to\nrun through. Each entry in the dictionary should be a list of transformers to apply sequentially.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.preprocessing import Subset\nfrom sklearn.preprocessing import StandardScaler\n\n# Map preprocessing cases through a dictionary\npreprocess_cases = {'none': [],\n                    'sc': [StandardScaler()],\n                    'sub': [Subset([0, 1])]\n                    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``fit`` methods determines automatically whether there is any preprocessing\nor any estimator jobs to run, so all we need to do is specify the arguments\nwe want to be processed. If a previous preprocessing job was fitted, those\npipelines are stored and will be used for subsequent estimator fits.\n\n.. currentmodule:: mlens.preprocessing\n\nThis can be helpful if the preprocessing is time-consuming, for instance if\nthe preprocessing pipeline is an ensemble itself. All ensembles implement\na ``transform`` method that, in contrast to the ``predict`` method, regenerates\nthe predictions made during the ``fit``call. More precisely, the ``transform``\nmethod uses the estimators fitted with cross-validation to construct predictions,\nwhereas the ``predict`` method uses the final estimators fitted on all data.\nThis allows us use ensembles as preprocessing steps that mimicks how that ensemble\nwould produce predictions for a subsequent meta learner or layer. Since fitting\nlarge ensembles is highly time-consuming, fixing the lower layers as preprocessing\ninput is highly valuable for tuning the higher layers and / or the final meta learner.\nSee the `model-selection-tutorial` tutorial for\nan example. To fit only the preprocessing pipelines, we omit any estimators in the\n``fit`` call.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator.fit(X, y, preprocessing=preprocess_cases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Selection across preprocessing pipelines\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo evaluate the same set of estimators across all pipelines with the same\nparameter distributions, there is no need to take any heed of the preprocessing\npipeline, just carry on as in the simple case::\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluator.fit(X, y, ests, params, n_iter=10)\nprint(\"Comparison across preprocessing pipelines:\\n\\n%r\" % evaluator.results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also map different estimators to different preprocessing folds, and\nmap different parameter distribution to each case. We will map two different\nparameter distributions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pars_1 = {'n_neighbors': randint(20, 30)}\npars_2 = {'n_neighbors': randint(2, 10)}\nparams = {('sc', 'knn'): pars_1,\n          ('none', 'knn'): pars_2,\n          ('sub', 'knn'): pars_2}\n\n# We can map different estimators to different cases\nests_1 = [('gnb', GaussianNB()), ('knn', KNeighborsClassifier())]\nests_2 = [('knn', KNeighborsClassifier())]\nestimators = {'sc': ests_1,\n              'none': ests_2,\n              'sub': ests_1}\n\nevaluator.fit(X, y, estimators, params, n_iter=10)\nprint(\"Comparison with different parameter dists:\\\\nn%r\" % evaluator.results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. py:currentmodule:: mlens.visualization\n\nVisualization guide\n-------------------\n\n**Explained variance plot**\n\nThe :class:`exp_var_plot` function\nplots the explained variance from mapping a matrix ``X`` onto a smaller\ndimension using a user-supplied transformer, such as the Scikit-learn\n:class:`sklearn.decomposition.PCA` transformer for\nPrincipal Components Analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom mlens.visualization import exp_var_plot\nfrom sklearn.decomposition import PCA\nexp_var_plot(X, PCA(), marker='s', where='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Principal Components Analysis plot**\n\nThe :class:`pca_plot` function\nplots a PCA analysis or similar if ``n_components`` is one of ``[1, 2, 3]``.\nBy passing a class labels, the plot shows how well separated different classes\nare.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.visualization import pca_plot\nfrom sklearn.decomposition import PCA\npca_plot(X, PCA(n_components=2), y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Principal Components Comparison plot**\n\nThe :class:`pca_comp_plot` function\nplots a matrix of PCA analyses, one for each combination of\n``n_components=2, 3`` and ``kernel='linear', 'rbf'``. ::\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.visualization import pca_comp_plot\npca_comp_plot (X, y)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Correlation matrix plot**\n\nThe :class:`corrmat` function plots the lower triangle of\na correlation matrix and is adapted the `Seaborn`_ correlation matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.visualization import corrmat\n# Generate som different predictions to correlate\nparams = [0.1, 0.3, 1.0, 3.0, 10, 30]\npreds = np.zeros((150, 6))\nfor i, c in enumerate(params):\n    preds[:, i] = LogisticRegression(C=c).fit(X, y).predict(X)\n\ncorr = DataFrame(preds, columns=['C=%.1f' % i for i in params]).corr()\ncorrmat(corr)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Clustered correlation heatmap plot**\n\nThe :class:`clustered_corrmap` function is similar to :class:`corrmat`,\nbut differs in two respects. First, and most importantly, it uses a user\nsupplied clustering estimator to cluster the correlation matrix on similar\nfeatures, which can often help visualize whether there are blocks of highly\ncorrelated features. Secondly, it plots the full matrix (as opposed to the\nlower triangle).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.visualization import clustered_corrmap\nfrom sklearn.cluster import KMeans\nZ = DataFrame(X, columns=['f_%i' %i for i in range(1, 5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We duplicate all features, note that the heatmap orders features\nas duplicate pairs, and thus fully pick up on this duplication.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corr = Z.join(Z, lsuffix='L', rsuffix='R').corr()\nclustered_corrmap(corr, KMeans())\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Input-Output correlations**\n\nThe :class:`corr_X_y` function gives a dashboard of\npairwise correlations between the input data (``X``) and the labels to be\npredicted (``y``). If the number of features is large, it is advised to set\nthe ``no_ticks`` parameter to ``True``, to avoid rendering an illegible\nx-axis. Note that ``X`` must be a :class:`pandas.DataFrame`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.visualization import corr_X_y\nZ = DataFrame(X, columns=['feature_%i' %i for i in range(1, 5)])\ncorr_X_y(Z, y, 2, no_ticks=False)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}